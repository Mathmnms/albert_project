{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22032c8a",
   "metadata": {},
   "source": [
    "# Scraping example (fake data)\n",
    "\n",
    "* Vous souhaitez inviter des auteurs à une conférence et devez récupérer leur nom sur ce [site](https://quotes.toscrape.com/), ainsi que leur citation pour mieux les connaître.\n",
    "* Écrivez un programme qui permet de récupérer tous les auteurs avec leur citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f288caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: bs4 in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (0.0.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "     ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/11.6 MB 3.9 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.5/11.6 MB 5.7 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.8/11.6 MB 6.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.2/11.6 MB 6.5 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.4/11.6 MB 6.5 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.5/11.6 MB 5.7 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.8/11.6 MB 5.7 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.1/11.6 MB 5.9 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.4/11.6 MB 6.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.8/11.6 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.1/11.6 MB 6.3 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.5/11.6 MB 6.4 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 3.8/11.6 MB 6.6 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.2/11.6 MB 6.8 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.6/11.6 MB 6.6 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 4.9/11.6 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.3/11.6 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.6/11.6 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 5.9/11.6 MB 6.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.2/11.6 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 6.5/11.6 MB 6.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.0/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 7.3/11.6 MB 7.1 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 7.7/11.6 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.0/11.6 MB 7.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 8.4/11.6 MB 7.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.8/11.6 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 9.2/11.6 MB 7.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 9.7/11.6 MB 7.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 10.2/11.6 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 10.6/11.6 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.0/11.6 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.3/11.6 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.6/11.6 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.6/11.6 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 11.6/11.6 MB 7.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Collecting numpy<2,>=1.22.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "     ---------------------------------------- 0.0/505.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 505.5/505.5 kB 10.5 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "     ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 345.4/345.4 kB 10.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ymeur\\desktop\\ml\\albert_project\\env\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.1 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests bs4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aef96369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import requests  # for making HTTP requests to web pages\n",
    "from bs4 import BeautifulSoup  # for parsing HTML content\n",
    "import pandas as pd\n",
    "\n",
    "# The base URL of the website we're scraping\n",
    "base_url = 'https://www.topachat.com/pages/produits_cat_est_micro_puis_rubrique_est_wgfx_pcie.html'\n",
    "\n",
    "def fetch_quotes(page_url):\n",
    "    \"\"\"\n",
    "    Fetch quotes and authors from a given page URL.\n",
    "\n",
    "    Parameters:\n",
    "    - page_url: URL of the page to scrape\n",
    "\n",
    "    Returns:\n",
    "    - A list of dictionaries, each containing an 'Author' and their 'Citation'\n",
    "    \"\"\"\n",
    "    cartes_graphiques = []  # Initialize an empty list to store quotes and authors\n",
    "    response = requests.get(page_url)  # Make a GET request to fetch the page content\n",
    "\n",
    "    if response.status_code == 200:  # Check if the request was successful (HTTP status code 200)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')  # Parse the HTML content of the page\n",
    "        cartes_graphiques = soup.find_all('product-list__product-wrapper')  # Find all quote blocks on the page\n",
    "\n",
    "        # for quote in quotes:  # Iterate over each quote block\n",
    "        #     text_element = quote.find('span', class_='text')  # Find the element containing the quote text\n",
    "        #     author_element = quote.find('small', class_='author')  # Find the element containing the author's name\n",
    "        #     if text_element and author_element:  # Ensure both elements were found\n",
    "        #         text = text_element.get_text(strip=True)  # Extract the text of the quote, stripping whitespace\n",
    "        #         author = author_element.get_text(strip=True)  # Extract the author's name, stripping whitespace\n",
    "        #         quotes_data.append({'Author': author, 'Citation': text})  # Add the quote and author to our list\n",
    "\n",
    "        # Check for a link to the next page\n",
    "        # next_page = soup.find('li', class_='next')\n",
    "        # if next_page and next_page.find('a'):  # Ensure the next page link exists\n",
    "        #     next_page_url = base_url + next_page.find('a')['href']  # Construct the URL for the next page\n",
    "        #     quotes_data.extend(fetch_quotes(next_page_url))  # Recursively fetch quotes from the next page\n",
    "        return cartes_graphiques\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        return None  # Return the list of quotes and authors\n",
    "\n",
    "# Start the scraping process from the first page of the site\n",
    "quotes_data = fetch_quotes(base_url)\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "# df = pd.DataFrame(quotes_data)\n",
    "\n",
    "# # Sort the DataFrame by the author names\n",
    "# df_sorted = df.sort_values(by='Author').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4c51c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb2322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.to_csv('../data/quotes.csv', index=False)  # Save the DataFrame to a CSV file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
